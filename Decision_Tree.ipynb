{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gini,1 depth\n",
      "Validating Accuracy: 67.89366053169734%\n",
      "\n",
      "gini,10 depth\n",
      "Validating Accuracy: 72.1881390593047%\n",
      "\n",
      "gini,50 depth\n",
      "Validating Accuracy: 75.4601226993865%\n",
      "\n",
      "gini,100 depth\n",
      "Validating Accuracy: 76.27811860940696%\n",
      "\n",
      "gini,200 depth\n",
      "Validating Accuracy: 76.89161554192229%\n",
      "\n",
      "gini,500 depth\n",
      "Validating Accuracy: 75.86912065439672%\n",
      "\n",
      "entropy,1 depth\n",
      "Validating Accuracy: 67.89366053169734%\n",
      "\n",
      "entropy,10 depth\n",
      "Validating Accuracy: 69.73415132924336%\n",
      "\n",
      "entropy,50 depth\n",
      "Validating Accuracy: 74.84662576687117%\n",
      "\n",
      "entropy,100 depth\n",
      "Validating Accuracy: 76.89161554192229%\n",
      "\n",
      "entropy,200 depth\n",
      "Validating Accuracy: 77.91411042944786%\n",
      "\n",
      "entropy,500 depth\n",
      "Validating Accuracy: 75.86912065439672%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import random\n",
    "import math\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "\n",
    " \n",
    " \n",
    "def load_data(filename, filename2):\n",
    "    \n",
    "    # Read Real File\n",
    "    f_real = open(filename,'r', encoding='UTF8')\n",
    "    rdr_real = csv.reader(f_real, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "    data_real=[]\n",
    "    \n",
    "    for line in rdr_real:\n",
    "        str = ' '.join(line)\n",
    "        data_real+=str.split(' |,')\n",
    "    f_real.close()\n",
    "    \n",
    "    \n",
    "    # Read Fake File\n",
    "    f_fake = open(filename2,'r', encoding='UTF8')\n",
    "    rdr = csv.reader(f_fake, delimiter='\\t', quoting=csv.QUOTE_NONE) \n",
    "    data_fake=[]\n",
    " \n",
    "    for line in rdr:\n",
    "        str = ' '.join(line)\n",
    "        data_fake+=str.split(' |,')        \n",
    "    f_fake.close()\n",
    "    \n",
    "    # Make Full Feature Dictionary(Vectorizer)\n",
    "    data_full = data_real+data_fake\n",
    "    vect = CountVectorizer()\n",
    "    \n",
    "    vect.fit(data_full)\n",
    "    feature_full = vect.vocabulary_  # Feature Vector and corresponding Feature Number    \n",
    "    counter_all = vect.transform(data_full)\n",
    "    result = counter_all.toarray()   # Indices for Each Senetence regarding to Feature Vectors\n",
    "        \n",
    "    \n",
    "    # Make Feature Name for Visualizing Decision Tree\n",
    "    names=[]\n",
    "    keys=[]\n",
    "    feature_name=[]\n",
    "    \n",
    "    for name,key in feature_full.items(): \n",
    "        names.append(name)\n",
    "        keys.append(key)\n",
    "\n",
    "    for i in range(len(feature_full)):  \n",
    "        feature_name.append(names[keys.index(i)])\n",
    "        \n",
    "            \n",
    "    label_all = np.zeros((np.shape(result)[0],1))\n",
    "    label_all[0:len(data_real)] = 1\n",
    "    \n",
    "    dictionary_final = np.concatenate((result, label_all), axis = 1) # Indices for Each Sentence + Label(1:Real, 0:Fake)\n",
    "        \n",
    "\n",
    "    # Split Dataset Randomly\n",
    "    sequence = [i for i in range(np.shape(dictionary_final)[0])]\n",
    "    idx_all=random.sample(sequence, np.shape(dictionary_final)[0])\n",
    "    \n",
    "    idx_train = math.floor(np.shape(dictionary_final)[0]*0.7)\n",
    "    idx_val = idx_train+math.floor(np.shape(dictionary_final)[0]*0.15)\n",
    "   \n",
    "    # 70%: Train, 15%: Validate, 15%: Test\n",
    "    subset_train = dictionary_final[idx_all[0:idx_train]]\n",
    "    subset_val =  dictionary_final[idx_all[idx_train:idx_val]]\n",
    "    subset_test =  dictionary_final[idx_all[idx_val:]]\n",
    "    \n",
    "    return subset_train, subset_val, subset_test, feature_name\n",
    "\n",
    "\n",
    "def select_model(train_data, validate_data, test_data):\n",
    "    \n",
    "    depth_list = [1, 10, 50, 100, 200, 500] # Max_depth\n",
    "    criterion_list=[\"gini\", \"entropy\"] # Criterion\n",
    "    \n",
    "    for i in range(len(criterion_list)):\n",
    "        for j in range(len(depth_list)):\n",
    "            DYTree = DecisionTreeClassifier(criterion=criterion_list[i], max_depth=depth_list[j])\n",
    "            DYTree = DYTree.fit(train_data[:,0:-1], train_data[:,-1])\n",
    "            validate_predict = DYTree.predict(validate_data[:,0:-1])\n",
    " \n",
    "            print(criterion_list[i]+\",\"+ str(depth_list[j])+\" depth\")\n",
    "            print(\"Validating Accuracy: \" + str(sum((validate_data[:,-1]-validate_predict)==0)/len(validate_data[:,-1])*100)+\"%\\n\")\n",
    "\n",
    "        \n",
    "# Main\n",
    "train_data, validate_data, test_data, name_feature = load_data('clean_real.txt', 'clean_fake.txt')\n",
    "select_model(train_data, validate_data, test_data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entropy, 200 depth\n",
      "Validating Accuracy: 77.0961145194274%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "Run DecisionTreeClassifier(highest validation accuracy) again to visualize\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "DYTree = DecisionTreeClassifier(criterion='entropy', max_depth=200)\n",
    "DYTree = DYTree.fit(train_data[:,0:-1], train_data[:,-1])\n",
    "validate_predict = DYTree.predict(validate_data[:,0:-1])\n",
    "\n",
    "print(\"entropy, 200 depth\")\n",
    "print(\"Validating Accuracy: \" + str(sum((validate_data[:,-1]-validate_predict)==0)/len(validate_data[:,-1])*100)+\"%\\n\")\n",
    "\n",
    "my_class_names = [\"Fake\", \"Real\"]\n",
    "tree.export_graphviz(DYTree, max_depth=2, out_file=\"TreeVisualize.dot\", feature_names= name_feature, class_names=my_class_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topmost split from the previous: the\n",
      "True number: 1917, where real: 1260, fake: 657\n",
      "False number: 369, where real: 114, fake: 255\n",
      "Headline(Class) is Real\n",
      "Entropy root is 0.9703331467646472\n",
      "Entropy left leaf is 0.9274006288148684\n",
      "Entropy right leaf is 0.8919496672696391\n",
      "Information Gain is 0.049\n",
      "\n",
      "Try other one: hillary\n",
      "True number: 2156, where real: 1356, fake: 800\n",
      "False number: 130, where real: 18, fake: 112\n",
      "Headline(Class) is Real\n",
      "Entropy root is 0.9703331467646472\n",
      "Entropy left leaf is 0.9514806205295638\n",
      "Entropy right leaf is 0.5801954953637369\n",
      "Information Gain is 0.04\n",
      "\n",
      "Try other one: trumps\n",
      "True number: 2126, where real: 1216, fake: 910\n",
      "False number: 160, where real: 158, fake: 2\n",
      "Headline(Class) is Real\n",
      "Entropy root is 0.9703331467646472\n",
      "Entropy left leaf is 0.9850041763214559\n",
      "Entropy right leaf is 0.09694460606247315\n",
      "Information Gain is 0.047\n",
      "\n",
      "Try other one: donald\n",
      "True number: 1542, where real: 796, fake: 746\n",
      "False number: 744, where real: 578, fake: 166\n",
      "Headline(Class) is Real\n",
      "Entropy root is 0.9703331467646472\n",
      "Entropy left leaf is 0.9992414365148061\n",
      "Entropy right leaf is 0.7658206523862816\n",
      "Information Gain is 0.047\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def compute_information_gain(dataset, standard, name_features):\n",
    "    loc_feature=name_features.index(standard)   \n",
    "    data_from_loc=dataset[:,loc_feature]\n",
    "    label=dataset[:,-1]\n",
    "    \n",
    "    true_real_number=0\n",
    "    true_fake_number=0\n",
    "    false_real_number=0\n",
    "    false_fake_number=0\n",
    "    true_number=0\n",
    "    false_number=0\n",
    "\n",
    "    for i in range(len(data_from_loc)):\n",
    "        if(data_from_loc[i] <= 0.5):\n",
    "            true_number=true_number+1\n",
    "            if(dataset[i,-1]==0):\n",
    "                true_fake_number=true_fake_number+1\n",
    "            else:\n",
    "                true_real_number=true_real_number+1\n",
    "        else:\n",
    "            false_number=false_number+1\n",
    "            if(dataset[i,-1]==0):\n",
    "                false_fake_number=false_fake_number+1\n",
    "            else:\n",
    "                false_real_number=false_real_number+1\n",
    "              \n",
    "    print(\"True number: {}, where real: {}, fake: {}\".format(true_number, true_real_number, true_fake_number))\n",
    "    print(\"False number: {}, where real: {}, fake: {}\".format(false_number, false_real_number, false_fake_number))\n",
    "    \n",
    "    root_real = true_real_number+false_real_number \n",
    "    root_fake = true_fake_number+false_fake_number\n",
    "    root_total = root_real+root_fake\n",
    "    \n",
    "    if(root_fake>root_real):\n",
    "        print(\"Headline(Class) is Fake\")\n",
    "    else:\n",
    "        print(\"Headline(Class) is Real\")\n",
    "\n",
    "    entropy_root = -((root_fake/root_total)*math.log2((root_fake/root_total))\n",
    "                     +(root_real/root_total)*math.log2((root_real/root_total)))\n",
    "    print(\"Entropy root is {}\".format(entropy_root))\n",
    "    entropy_left_leaf= -((true_fake_number/true_number)*math.log2((true_fake_number/true_number))\n",
    "                         +(true_real_number/true_number)*math.log2((true_real_number/true_number)))\n",
    "    print(\"Entropy left leaf is {}\".format(entropy_left_leaf))\n",
    "    entropy_right_leaf= -((false_fake_number/false_number)*math.log2((false_fake_number/false_number))\n",
    "                          +(false_real_number/false_number)*math.log2((false_real_number/false_number)))\n",
    "    print(\"Entropy right leaf is {}\".format(entropy_right_leaf))\n",
    "\n",
    "    IG = entropy_root-((true_number/root_total)*entropy_left_leaf\n",
    "                       +(false_number/root_total)*entropy_right_leaf)\n",
    "    \n",
    "    \n",
    "    print(\"Information Gain is {}\\n\".format(round(IG,3)))\n",
    "    \n",
    "        \n",
    "print(\"Topmost split from the previous: the\")\n",
    "compute_information_gain(train_data,\"the\",name_feature)\n",
    "print(\"Try other one: hillary\")\n",
    "compute_information_gain(train_data,\"hillary\",name_feature)\n",
    "print(\"Try other one: trumps\")\n",
    "compute_information_gain(train_data,\"trumps\",name_feature)\n",
    "print(\"Try other one: donald\")\n",
    "compute_information_gain(train_data,\"donald\",name_feature)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
